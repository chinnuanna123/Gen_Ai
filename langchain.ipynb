{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0iL6jpV2x4oOqrdRZN/RH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chinnuanna123/Gen_Ai/blob/master/langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain huggingface_hub\n"
      ],
      "metadata": {
        "id": "_PKGhpvk375Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-community\n"
      ],
      "metadata": {
        "id": "MzzVHAFZsL34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain_community.llms import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "LfkxxfExrx0w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get api token from userdata"
      ],
      "metadata": {
        "id": "U2URaMFBqTwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = userdata.get('hub_key')"
      ],
      "metadata": {
        "id": "f9zAfVoZqYkP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set environment variable"
      ],
      "metadata": {
        "id": "b2y18MRFqkNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['hub_key'] = key"
      ],
      "metadata": {
        "id": "kicrw4vQqwik"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define Hugggingface model"
      ],
      "metadata": {
        "id": "o7uddt-KrByU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")"
      ],
      "metadata": {
        "id": "wCzwAd-qrFix"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define a prompt template"
      ],
      "metadata": {
        "id": "bJx2dth5sCSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate.from_template(\"Generate a short story about {topic}\")\n"
      ],
      "metadata": {
        "id": "0k7IEelisDtE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Use RunnableSequence"
      ],
      "metadata": {
        "id": "Wy9xvoxzsJ9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm\n"
      ],
      "metadata": {
        "id": "-t2gUit3sQLf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Invoke the chain with user input"
      ],
      "metadata": {
        "id": "qW_ol73DsVnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"topic\": \"Nature\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSjfy_BLsbIs",
        "outputId": "68146d6d-e6ab-4da3-d46e-c782d147b52b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " vs. Nurture.\n",
            "\n",
            "Title: The Orchid and the Oak\n",
            "\n",
            "In the heart of an ancient forest, two seeds lay side by side, nestled in the decaying leaves of last autumn. One was a delicate, twisted speck - an orchid seed, the other a robust, acorn-like nut - an oak seed. Both were products of nature, yet their fates were destined to be shaped by nurture.\n",
            "\n",
            "The first raindrops of spring awakened the orchid seed. It sprouted a tender root, seeking moisture and nutrients in the forest floor. It was nurtured by the damp, rich soil and the dappled sunlight that filtered through the canopy above. The orchid grew swiftly, its leaves unfurling like tiny green scrolls, its stalk reaching towards the light. It was a beautiful, fragile thing, its petals as delicate as a whisper, its scent sweet and heady.\n",
            "\n",
            "Meanwhile, the oak seed remained dormant, biding its time. When the first frost of winter finally cracked its hard shell, the acorn sent out a strong, taproot, anchoring itself deeply into the earth. It grew slowly, steadily, its leaves broad and sturdy, designed to capture the maximum sunlight. The oak was nurtured by the strength of the earth, its roots delving deep to find water and nutrients. It was a resilient, powerful thing, its trunk thick and strong, its branches reaching out like arms, ready to weather any storm.\n",
            "\n",
            "The orchid and the oak grew side by side, their roots intertwining, their leaves brushing against each other. They were as different as night and day, yet they coexisted peacefully, each in its own way thriving in the nurturing embrace of the forest.\n",
            "\n",
            "One day, a great storm swept through the forest. The wind howled, and the rain poured down in torrents. The orchid, with its slender stalk and delicate petals, was battered by the storm. Its leaves were torn, its stalk was bent, and its petals were bruised. It seemed that the orchid would not survive the night.\n",
            "\n",
            "The oak, on the other hand, stood tall and strong against the storm. Its broad leaves sheltered the smaller plants beneath it, and its sturdy trunk bent but did not break. When the storm finally passed, the oak stood tall and proud, its leaves glistening with rain, its roots firmly anchored in the earth.\n",
            "\n",
            "The following spring, the orchid did not bloom. Its stalk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SimpleSequentialChain in LangChain"
      ],
      "metadata": {
        "id": "r3STzcWtvjj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_huggingface\n"
      ],
      "metadata": {
        "id": "cgiGcPb_WWo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain langchain_community huggingface_hub\n"
      ],
      "metadata": {
        "id": "2cL3xJe1YbdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "RmqF82kLmoXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "QyIVAGnUmKcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the API token from userdata"
      ],
      "metadata": {
        "id": "LvZ_3MjjuJhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = userdata.get('hub_key')\n"
      ],
      "metadata": {
        "id": "4xmOFnDauNmm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set environment variable"
      ],
      "metadata": {
        "id": "tALdOtQnuXhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['hub_key'] = key\n"
      ],
      "metadata": {
        "id": "r1D33vB9ubkD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Hugging Face model"
      ],
      "metadata": {
        "id": "PvnunDIaukRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"google/gemma-2-2b-it\",\n",
        "    task=\"text-generation\"\n",
        ")"
      ],
      "metadata": {
        "id": "b2xRlaetup-_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define prompt templates"
      ],
      "metadata": {
        "id": "uNP-aaOMneUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a catchy name for a product that is {product}?\"\n",
        ")\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"product_name\"],\n",
        "    template=\"Write a short, catchy slogan for {product_name}.\"\n",
        ")\n",
        "\n",
        "prompt_3 = PromptTemplate(\n",
        "    input_variables=[\"slogan\"],\n",
        "    template=\"Write a short marketing description that includes the following slogan: {slogan}\"\n",
        ")"
      ],
      "metadata": {
        "id": "wx8KpQpcnfMz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrap Hugging Face endpoint with LLMChain"
      ],
      "metadata": {
        "id": "G3Rbeo3unvGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"product_name\")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2, output_key=\"slogan\")\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt_3, output_key=\"description\")"
      ],
      "metadata": {
        "id": "5MxkzX1Gn0uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4627d1-adea-4dc5-9f20-14c87b1872b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-03f1f050fcf6>:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"product_name\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the SequentialChain"
      ],
      "metadata": {
        "id": "xMq4N3h0n8ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_1, chain_2, chain_3],\n",
        "    input_variables=[\"product\"],  # First input\n",
        "    output_variables=[\"product_name\", \"slogan\", \"description\"],  # Final outputs\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "0vXggKm5oBah"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example input"
      ],
      "metadata": {
        "id": "-1t_KH06oL5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product = \"a new type of eco-friendly water bottle\"\n",
        "result = overall_chain.invoke({\"product\": product})\n",
        "print(f\"Product: {product}\")\n",
        "print(f\"Product Name: {result['product_name']}\")\n",
        "print(f\"Slogan: {result['slogan']}\")\n",
        "print(f\"Description: {result['description']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZcUmy14pa97",
        "outputId": "692f78e5-01af-4724-9fb3-644a6c1385a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Product: a new type of eco-friendly water bottle\n",
            "Product Name:  I'm looking for a brand name for a new product, a water bottle made of recycled materials that filters out contaminants from tap water and keeps water cold for up to 24 hours. I'd like to avoid words that are too common, or words that are commonly used in other contexts.\n",
            "\n",
            "The product has a sleek, modern, high-tech design, and it's eco-friendly. The bottle itself is not too heavy, and it has a simple, intuitive design that is easy to use.\n",
            "\n",
            "I'm looking for a brand name that reflects the following qualities:\n",
            "\n",
            "1. Eco-friendliness\n",
            "2. Innovation and advanced technology\n",
            "3. Simplicity and ease of use\n",
            "4. Sustainability\n",
            "5. Modern design and aesthetics\n",
            "\n",
            "Ideally, the name should be short and catchy, easy to remember, and it should roll off the tongue. It would be great if the name could convey the product's unique features and benefits in a simple and memorable way. I'm open to both literal and metaphorical names, as long as they fit the criteria above.\n",
            "\n",
            "Here are a few examples of names that I like, but I'm looking for something better:\n",
            "\n",
            "1. AquaPure\n",
            "2. EcoAqua\n",
            "3. GreenSip\n",
            "4. SustainSip\n",
            "5. PureFlow\n",
            "6. ECOcool\n",
            "7. AquaChill\n",
            "8. EcoFresh\n",
            "9. FreshFlow\n",
            "10. EcoPure\n",
            "\n",
            "Please provide me with a list of 10-15 brand names that fit the criteria above. Thank you!\n",
            "\n",
            "(If you're up for the challenge, I'd also love to see your reasoning behind why you think a particular name is a good fit for the product. This will help me understand what makes a name effective, and it will help me make a more informed decision. Thank you!)\n",
            "Slogan: \n",
            "Description:  \"Save time, save money, save the world!\"\n",
            "\n",
            "> **GreenClean™**\n",
            ">\n",
            "> Discover the revolutionary eco-friendly cleaning solution that's not just cleaning your home, but also cleaning up your carbon footprint. With **GreenClean™**, you're saving time with our quick and effective formulas, saving money with our long-lasting, concentrated products, and saving the world with our 100% biodegradable, non-toxic ingredients. Join the sustainable cleaning revolution today and **save time, save money, save the world!**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain OutputParser"
      ],
      "metadata": {
        "id": "RBDCe0iJyP68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain-community"
      ],
      "metadata": {
        "id": "Ypb576K50XA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import os"
      ],
      "metadata": {
        "id": "H379HC13xBOp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the API token from userdata"
      ],
      "metadata": {
        "id": "Zo5JGppszDxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = userdata.get('hub_key')"
      ],
      "metadata": {
        "id": "oYkWx2XXy6l0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set environment variable"
      ],
      "metadata": {
        "id": "A2I08FqMzM4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['hub_key'] = key\n"
      ],
      "metadata": {
        "id": "BJUz_qJ2zOCr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load Hugging Face model"
      ],
      "metadata": {
        "id": "L-lWGUNNxNCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "      repo_id=\"google/gemma-2-2b-it\",\n",
        "      task=\"text-generation\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfbc6QuaxMnt",
        "outputId": "a7438d1d-2ed8-439f-d7bb-01f4230491cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4c1be2100763>:1: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceEndpoint(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define list output parser"
      ],
      "metadata": {
        "id": "FW-Mc0q7xcEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = CommaSeparatedListOutputParser()\n"
      ],
      "metadata": {
        "id": "_IGv6R-Lxc-D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define prompt template"
      ],
      "metadata": {
        "id": "w8KPKnLgxqvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"List 5 programming languages, separated by commas.\"\n",
        ")"
      ],
      "metadata": {
        "id": "7sLhKt0yxsEx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create chain"
      ],
      "metadata": {
        "id": "gj9_Dj_lx7Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser\n"
      ],
      "metadata": {
        "id": "JW7CF5tpyAof"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoke chain"
      ],
      "metadata": {
        "id": "pNlh-NClyIj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0dFdvwZyJjL",
        "outputId": "b26a31f4-6a82-4d15-a050-0ec794d0dbce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"It doesn't matter which ones. Here are five:\", 'Java', 'Python', 'C++', 'PHP', 'JavaScript']\n"
          ]
        }
      ]
    }
  ]
}